import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import sounddevice as sd
import time
from collections import deque
import threading
import sys
import math

# –ü—É—Ç–∏
MODEL_PATH = "model/my_sound_model.h5"
yamnet_model = hub.load("https://tfhub.dev/google/yamnet/1")
classifier = tf.keras.models.load_model(MODEL_PATH)
class_names = ["negative", "positive"]

SAMPLE_RATE = 16000
BLOCK_DURATION = 1.0
BLOCK_SIZE = int(SAMPLE_RATE * BLOCK_DURATION)
MIN_CONFIDENCE = 0.8
SMOOTH_WINDOW = 3
RMS_THRESHOLD = 0.01
CHANNELS = 1  # –ú–æ–Ω–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω

# –ë—É—Ñ–µ—Ä—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
prediction_buffer = deque(maxlen=SMOOTH_WINDOW)
volume_buffer = deque(maxlen=30)
frequency_buffer = deque(maxlen=20)
direction_estimate_buffer = deque(maxlen=10)
last_prediction = None
last_time = 0
is_running = True

class MonoDirectionEstimator:
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.prev_audio = None
        self.movement_history = deque(maxlen=20)
        
    def estimate_direction_from_movement(self, audio_data):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∑–≤—É–∫–∞
        (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ/—É–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
        """
        try:
            if self.prev_audio is None:
                self.prev_audio = audio_data.copy()
                return 0.0, "–ù–ï–ü–û–î–í–ò–ñ–ù–û"
            
            # –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏
            current_rms = np.sqrt(np.mean(audio_data**2))
            prev_rms = np.sqrt(np.mean(self.prev_audio**2))
            
            volume_change = current_rms - prev_rms
            
            # –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–ø–µ–∫—Ç—Ä–∞
            current_spectrum = np.abs(np.fft.fft(audio_data))[:len(audio_data)//2]
            prev_spectrum = np.abs(np.fft.fft(self.prev_audio))[:len(self.prev_audio)//2]
            
            # –¶–µ–Ω—Ç—Ä —Ç—è–∂–µ—Å—Ç–∏ —Å–ø–µ–∫—Ç—Ä–∞ (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —á–∞—Å—Ç–æ—Ç—ã)
            freqs = np.fft.fftfreq(len(audio_data), 1/self.sample_rate)[:len(audio_data)//2]
            
            current_centroid = np.sum(freqs * current_spectrum) / (np.sum(current_spectrum) + 1e-10)
            prev_centroid = np.sum(freqs * prev_spectrum) / (np.sum(prev_spectrum) + 1e-10)
            
            centroid_change = current_centroid - prev_centroid
            
            # –≠—Ñ—Ñ–µ–∫—Ç –î–æ–ø–ª–µ—Ä–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π)
            doppler_estimate = centroid_change / prev_centroid if prev_centroid > 0 else 0
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            if abs(volume_change) > 0.005:  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
                if volume_change > 0:
                    direction_desc = "–ü–†–ò–ë–õ–ò–ñ–ê–ï–¢–°–Ø"
                    movement_score = min(1.0, volume_change * 100)
                else:
                    direction_desc = "–£–î–ê–õ–Ø–ï–¢–°–Ø"
                    movement_score = max(-1.0, volume_change * 100)
            else:
                direction_desc = "–°–¢–ê–ë–ò–õ–¨–ù–û"
                movement_score = 0.0
            
            self.movement_history.append(movement_score)
            self.prev_audio = audio_data.copy()
            
            return movement_score, direction_desc
            
        except Exception as e:
            return 0.0, "–û–®–ò–ë–ö–ê"
    
    def analyze_frequency_distribution(self, audio_data):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        try:
            # FFT –∞–Ω–∞–ª–∏–∑
            fft = np.abs(np.fft.fft(audio_data))
            freqs = np.fft.fftfreq(len(audio_data), 1/self.sample_rate)
            
            # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã
            positive_freqs = freqs[:len(freqs)//2]
            positive_fft = fft[:len(fft)//2]
            
            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –ø–æ–ª–æ—Å—ã
            low_band = np.sum(positive_fft[(positive_freqs >= 80) & (positive_freqs < 300)])
            mid_band = np.sum(positive_fft[(positive_freqs >= 300) & (positive_freqs < 2000)])
            high_band = np.sum(positive_fft[(positive_freqs >= 2000) & (positive_freqs < 8000)])
            
            total_energy = low_band + mid_band + high_band
            
            if total_energy > 0:
                low_ratio = low_band / total_energy
                mid_ratio = mid_band / total_energy
                high_ratio = high_band / total_energy
                
                # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç–æ—Ç–∞
                dominant_freq_idx = np.argmax(positive_fft)
                dominant_freq = positive_freqs[dominant_freq_idx] if dominant_freq_idx < len(positive_freqs) else 0
                
                return {
                    'low_ratio': low_ratio,
                    'mid_ratio': mid_ratio,
                    'high_ratio': high_ratio,
                    'dominant_freq': dominant_freq,
                    'total_energy': total_energy
                }
            else:
                return None
                
        except Exception:
            return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ü–µ–Ω—â–∏–∫ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
direction_estimator = MonoDirectionEstimator()

def extract_embedding_from_audio(audio):
    scores, embeddings, spectrogram = yamnet_model(audio)
    embedding = tf.reduce_mean(embeddings, axis=0)
    return embedding

def smooth_predictions(new_prediction):
    prediction_buffer.append(new_prediction)
    if len(prediction_buffer) < SMOOTH_WINDOW:
        return None
    
    negative_count = sum(1 for p in prediction_buffer if p == 0)
    positive_count = sum(1 for p in prediction_buffer if p == 1)
    return 0 if negative_count >= positive_count else 1

def create_movement_visualizer(movement_score, movement_desc, width=40):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–≤–∏–∂–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–≤—É–∫–∞"""
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score –æ—Ç -1 –¥–æ 1
    normalized = np.clip(movement_score, -1, 1)
    
    # –¶–µ–Ω—Ç—Ä –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    center = width // 2
    pos = center + int(normalized * (width // 4))
    pos = np.clip(pos, 0, width - 1)
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    bar = list("‚ñë" * width)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è
    if normalized > 0.2:  # –ü—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è
        for i in range(center, min(width, pos + 3)):
            bar[i] = "‚ñ∂" if i == pos else "‚ñ∫"
    elif normalized < -0.2:  # –£–¥–∞–ª—è–µ—Ç—Å—è
        for i in range(max(0, pos - 2), center + 1):
            bar[i] = "‚óÑ" if i == pos else "‚óÇ"
    else:  # –°—Ç–∞–±–∏–ª—å–Ω–æ
        bar[pos] = "‚¨§"
    
    # –ú–µ—Ç–∫–∏
    bar[0] = "‚óÑ"
    bar[center] = "‚óè"
    bar[width - 1] = "‚ñ∫"
    
    return f"|{(''.join(bar))}| {movement_desc}"

def create_frequency_analysis_display(freq_analysis):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç"""
    if freq_analysis is None:
        return "üéµ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —á–∞—Å—Ç–æ—Ç–∞—Ö"
    
    low = freq_analysis['low_ratio']
    mid = freq_analysis['mid_ratio'] 
    high = freq_analysis['high_ratio']
    dominant = freq_analysis['dominant_freq']
    
    # –í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å—ã
    low_bar = "‚ñà" * int(low * 20)
    mid_bar = "‚ñà" * int(mid * 20)
    high_bar = "‚ñà" * int(high * 20)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–≤—É–∫–∞
    if mid > 0.6:
        sound_type = "üó£Ô∏è –†–ï–ß–¨/–ì–û–õ–û–°"
    elif high > 0.5:
        sound_type = "üîî –í–´–°–û–ö–ò–ï –ß–ê–°–¢–û–¢–´"
    elif low > 0.5:
        sound_type = "ü•Å –ù–ò–ó–ö–ò–ï –ß–ê–°–¢–û–¢–´"
    else:
        sound_type = "üéµ –°–ú–ï–®–ê–ù–ù–´–ô"
    
    result = f"""
üéµ === –ß–ê–°–¢–û–¢–ù–´–ô –ê–ù–ê–õ–ò–ó ===
   –ù–∏–∑–∫–∏–µ (80-300 Hz):    |{low_bar:<20}| {low:.2f}
   –°—Ä–µ–¥–Ω–∏–µ (300-2000 Hz): |{mid_bar:<20}| {mid:.2f}
   –í—ã—Å–æ–∫–∏–µ (2-8 kHz):     |{high_bar:<20}| {high:.2f}
   –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è: {dominant:.0f} Hz
   –¢–∏–ø –∑–≤—É–∫–∞: {sound_type}
"""
    return result

def create_3d_sound_map(movement_score, freq_analysis, volume):
    """–°–æ–∑–¥–∞–µ—Ç 3D –∫–∞—Ä—Ç—É –∑–≤—É–∫–∞"""
    if freq_analysis is None:
        return "üó∫Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—Ä—Ç—ã"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if movement_score > 0.3:
        position = "üîä –ü–†–ò–ë–õ–ò–ñ–ê–Æ–©–ò–ô–°–Ø –ò–°–¢–û–ß–ù–ò–ö"
        distance = "–ë–ª–∏–∑–∫–æ"
    elif movement_score < -0.3:
        position = "üîá –£–î–ê–õ–Ø–Æ–©–ò–ô–°–Ø –ò–°–¢–û–ß–ù–ò–ö"  
        distance = "–î–∞–ª–µ–∫–æ"
    else:
        position = "üìç –°–¢–ê–¶–ò–û–ù–ê–†–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö"
        distance = "–°—Ä–µ–¥–Ω–µ"
    
    # –û—Ü–µ–Ω–∫–∞ –≤—ã—Å–æ—Ç—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–∞–º
    if freq_analysis['high_ratio'] > 0.4:
        height = "‚¨ÜÔ∏è –í–´–°–û–ö–û (–º–Ω–æ–≥–æ –í–ß)"
    elif freq_analysis['low_ratio'] > 0.4:
        height = "‚¨áÔ∏è –ù–ò–ó–ö–û (–º–Ω–æ–≥–æ –ù–ß)"
    else:
        height = "‚û°Ô∏è –ù–ê –£–†–û–í–ù–ï"
    
    # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å
    if volume > 0.05:
        intensity = "üî• –ì–†–û–ú–ö–ò–ô"
    elif volume > 0.02:
        intensity = "üîÜ –°–†–ï–î–ù–ò–ô"
    else:
        intensity = "üí° –¢–ò–•–ò–ô"
    
    return f"""
üó∫Ô∏è === 3D –ö–ê–†–¢–ê –ó–í–£–ö–ê ===
   –ü–æ–∑–∏—Ü–∏—è: {position}
   –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance}
   –í—ã—Å–æ—Ç–∞: {height}
   –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å: {intensity}
"""

def movement_stats_display():
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–≤–∏–∂–µ–Ω–∏—è"""
    while is_running:
        if direction_estimate_buffer and frequency_buffer:
            recent_movements = list(direction_estimate_buffer)[-10:]
            recent_freqs = list(frequency_buffer)[-5:]
            
            if recent_movements and recent_freqs:
                avg_movement = np.mean(recent_movements)
                movement_stability = np.std(recent_movements)
                
                # –û–±—â–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è
                if avg_movement > 0.1:
                    trend = "üìà –ò–°–¢–û–ß–ù–ò–ö –ü–†–ò–ë–õ–ò–ñ–ê–ï–¢–°–Ø"
                elif avg_movement < -0.1:
                    trend = "üìâ –ò–°–¢–û–ß–ù–ò–ö –£–î–ê–õ–Ø–ï–¢–°–Ø"
                else:
                    trend = "üìä –ò–°–¢–û–ß–ù–ò–ö –°–¢–ê–ë–ò–õ–ï–ù"
                
                # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
                if movement_stability < 0.1:
                    stability = "üü¢ –°–¢–ê–ë–ò–õ–¨–ù–û"
                elif movement_stability < 0.3:
                    stability = "üü° –£–ú–ï–†–ï–ù–ù–û"
                else:
                    stability = "üî¥ –ù–ï–°–¢–ê–ë–ò–õ–¨–ù–û"
                
                latest_freq = recent_freqs[-1] if recent_freqs else None
                
                print(f"\nüìä === –ê–ù–ê–õ–ò–ó –î–í–ò–ñ–ï–ù–ò–Ø (–ú–û–ù–û –ú–ò–ö–†–û–§–û–ù) ===")
                print(f"   –¢–µ–Ω–¥–µ–Ω—Ü–∏—è: {trend}")
                print(f"   –°—Ä–µ–¥–Ω–∏–π —Å–¥–≤–∏–≥: {avg_movement:.3f}")
                print(f"   –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {stability}")
                
                if latest_freq:
                    print(create_frequency_analysis_display(latest_freq))
                    print(create_3d_sound_map(avg_movement, latest_freq, volume_buffer[-1] if volume_buffer else 0))
                
                print("=" * 60)
                
                for _ in range(3):
                    print()
        
        time.sleep(6)

def volume_display_thread():
    """–ü–æ—Ç–æ–∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
    while is_running:
        if volume_buffer and direction_estimate_buffer:
            current_vol = volume_buffer[-1]
            movement_score = direction_estimate_buffer[-1] if direction_estimate_buffer else 0
            
            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥—Ä–æ–º–∫–æ—Å—Ç–∏
            vol_normalized = min(1.0, np.log10(current_vol * 1000 + 1) / 3) if current_vol > 0 else 0
            vol_bar = "‚ñà" * int(vol_normalized * 30) + "‚ñë" * (30 - int(vol_normalized * 30))
            
            # –î–≤–∏–∂–µ–Ω–∏–µ
            movement_desc = "–ü–†–ò–ë–õ–ò–ñ–ê–ï–¢–°–Ø" if movement_score > 0.1 else "–£–î–ê–õ–Ø–ï–¢–°–Ø" if movement_score < -0.1 else "–°–¢–ê–ë–ò–õ–¨–ù–û"
            movement_viz = create_movement_visualizer(movement_score, movement_desc)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            sys.stdout.write(f"\rüîä |{vol_bar}| {current_vol:.3f}")
            sys.stdout.write(f"\nüéØ {movement_viz}")
            sys.stdout.flush()
        
        time.sleep(0.3)

def audio_callback(indata, frames, time_info, status):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–Ω–æ –∞—É–¥–∏–æ —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–≤–∏–∂–µ–Ω–∏—è"""
    global last_prediction, last_time
    
    if status:
        print(f"\n‚ö†Ô∏è Audio status: {status}")
    
    try:
        # –ú–æ–Ω–æ –∞—É–¥–∏–æ
        mono = indata[:, 0].astype(np.float32)
        rms = np.sqrt(np.mean(mono**2))
        
        volume_buffer.append(rms)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–≤–∏–∂–µ–Ω–∏—è –∏ —á–∞—Å—Ç–æ—Ç
        if rms > RMS_THRESHOLD:
            movement_score, movement_desc = direction_estimator.estimate_direction_from_movement(mono)
            freq_analysis = direction_estimator.analyze_frequency_distribution(mono)
            
            direction_estimate_buffer.append(movement_score)
            if freq_analysis:
                frequency_buffer.append(freq_analysis)
        
        if rms < RMS_THRESHOLD:
            return
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        emb = extract_embedding_from_audio(mono)
        pred = classifier.predict(np.expand_dims(emb, axis=0), verbose=0)[0]
        label_idx = np.argmax(pred)
        confidence = pred[label_idx]
        
        smoothed_prediction = smooth_predictions(label_idx)
        
        current_time = time.time()
        if (smoothed_prediction is not None and 
            confidence > MIN_CONFIDENCE and 
            smoothed_prediction != last_prediction and
            current_time - last_time > 1.5):
            
            label = class_names[smoothed_prediction]
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–≤–∏–∂–µ–Ω–∏–∏
            if direction_estimate_buffer and frequency_buffer:
                current_movement = direction_estimate_buffer[-1]
                movement_desc = "–ü–†–ò–ë–õ–ò–ñ–ê–ï–¢–°–Ø" if current_movement > 0.1 else "–£–î–ê–õ–Ø–ï–¢–°–Ø" if current_movement < -0.1 else "–°–¢–ê–ë–ò–õ–¨–ù–û"
                latest_freq = frequency_buffer[-1]
            else:
                current_movement = 0
                movement_desc = "–ù–ï–ò–ó–í–ï–°–¢–ù–û"
                latest_freq = None
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            sys.stdout.write("\r" + " " * 100 + "\r")
            
            result_emoji = "üî¥" if label == "negative" else "üü¢"
            print(f"\n{result_emoji} === –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï (–ú–û–ù–û –ê–ù–ê–õ–ò–ó) === {result_emoji}")
            print(f"üéß –ö–ª–∞—Å—Å:         {label.upper()}")
            print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:   {confidence:.2f} ({confidence*100:.1f}%)")
            print(f"üîä –ì—Ä–æ–º–∫–æ—Å—Ç—å:     {rms:.3f}")
            print(f"üéØ –î–≤–∏–∂–µ–Ω–∏–µ:      {movement_desc} ({current_movement:.3f})")
            
            if latest_freq:
                print(f"üéµ –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç–æ—Ç–∞: {latest_freq['dominant_freq']:.0f} Hz")
                
                # –¢–∏–ø –∑–≤—É–∫–∞
                if latest_freq['mid_ratio'] > 0.6:
                    sound_type = "üó£Ô∏è –†–ï–ß–¨/–ì–û–õ–û–°"
                elif latest_freq['high_ratio'] > 0.5:
                    sound_type = "üîî –í–´–°–û–ö–ò–ï –¢–û–ù–ê"
                elif latest_freq['low_ratio'] > 0.5:
                    sound_type = "ü•Å –ù–ò–ó–ö–ò–ï –¢–û–ù–ê"
                else:
                    sound_type = "üéµ –°–ú–ï–®–ê–ù–ù–´–ô"
                    
                print(f"üéº –¢–∏–ø –∑–≤—É–∫–∞:     {sound_type}")
            
            print(f"‚è∞ –í—Ä–µ–º—è:         {time.strftime('%H:%M:%S')}")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è
            movement_viz = create_movement_visualizer(current_movement, movement_desc)
            print(f"üìç –ê–Ω–∞–ª–∏–∑:        {movement_viz}")
            
            print("=" * 60)
            
            last_prediction = smoothed_prediction
            last_time = current_time
            
    except Exception as e:
        print(f"\n‚ùå Error in audio callback: {e}")

def main():
    print("üéôÔ∏è === –î–ï–¢–ï–ö–¢–û–† –î–õ–Ø –ú–û–ù–û –ú–ò–ö–†–û–§–û–ù–ê ===")
    print("üéØ –ê–Ω–∞–ª–∏–∑ –¥–≤–∏–∂–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–≤—É–∫–∞")
    print("üéµ –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    devices = sd.query_devices()
    print("üé§ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:")
    for i, device in enumerate(devices):
        if device['max_input_channels'] >= 1:
            print(f"   {i}: {device['name']} ‚úÖ")
    
    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   –†–µ–∂–∏–º: –ú–û–ù–û –º–∏–∫—Ä–æ—Ñ–æ–Ω")
    print(f"   –ê–Ω–∞–ª–∏–∑: –î–≤–∏–∂–µ–Ω–∏–µ + –ß–∞—Å—Ç–æ—Ç—ã")
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {MIN_CONFIDENCE}")
    print(f"   –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ: {SMOOTH_WINDOW} –∫–∞–¥—Ä–æ–≤")
    print(f"   –ü–æ—Ä–æ–≥ —à—É–º–∞: {RMS_THRESHOLD}")
    print("=" * 50)
    print("üéØ –ì–æ—Ç–æ–≤ –∫ –∞–Ω–∞–ª–∏–∑—É! –ì–æ–≤–æ—Ä–∏—Ç–µ –∏–ª–∏ –∏–∑–¥–∞–≤–∞–π—Ç–µ –∑–≤—É–∫–∏...")
    print("üí° –î–≤–∏–≥–∞–π—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–≤—É–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–≤–∏–∂–µ–Ω–∏—è")
    print("(Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞)\n")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏
    volume_thread = threading.Thread(target=volume_display_thread, daemon=True)
    volume_thread.start()
    
    stats_thread = threading.Thread(target=movement_stats_display, daemon=True)
    stats_thread.start()
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∞—É–¥–∏–æ –ø–æ—Ç–æ–∫
    try:
        with sd.InputStream(channels=CHANNELS, samplerate=SAMPLE_RATE, 
                          callback=audio_callback, blocksize=BLOCK_SIZE):
            try:
                while True:
                    time.sleep(0.1)
            except KeyboardInterrupt:
                global is_running
                is_running = False
                sys.stdout.write("\r" + " " * 100 + "\r")
                print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞...")
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞—É–¥–∏–æ: {e}")

if __name__ == "__main__":
    main()